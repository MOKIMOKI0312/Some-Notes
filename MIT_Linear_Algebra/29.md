
## 28. Similiar matrices and Jordan form
- positive definite matrix: 
$$
x^T A x \text{  always}> 0
$$
> except x = 0
If A,B are pos def, then $x^T (A+B)x$ is pos def.


#### Similiar matrices
$A$ m by n
$$
A^T A 
$$
is square，symmetric and positive definite
- prove:we need to prove $x^T A^T A x > 0$
$$
x^T A^T A x = (Ax)^T Ax = ||Ax||^2 \geq 0
$$ 

(n by n)A and B are similiar if :FOR SOME M
$$
B = M^{-1} A M
$$

Example:
if A has eigenvecotrs $S$
$$
S^{-1} A S = \Lambda
$$
now A is similiar to $\Lambda$
$$
A = 
\begin{bmatrix}
2 & 1 \\
1 & 2
\end{bmatrix}, \Lambda =
\begin{bmatrix}
3 & 0 \\
0 & 1
\end{bmatrix}
$$
$$
\begin{bmatrix}
1 & -4 \\
0 & 1
\end{bmatrix}
\begin{bmatrix}
2 & 1 \\
1 & 2
\end{bmatrix}
\begin{bmatrix}
1 & 4 \\
0 & 1
\end{bmatrix} =
\begin{bmatrix}
-2 & -15 \\
1 & 6
\end{bmatrix}
$$
- Main fact: A and $\Lambda$ have the same **eigenvalues**

#### Similiar matrices have the same eigenvalues
and have some M to connect them

$$
Ax = \lambda x \rightarrow M^{-1} A M M^{-1} x = \lambda M^{-1} x \rightarrow B(M^{-1} x) = \lambda (M^{-1} x)
$$
> but the eigenvectors are different


#### For bad cases(lambda1 = lambda2)
if $\lambda_1 = \lambda_2 = 4$,then eigenvectors are not perpendicular

One family has 
$$
\begin{bmatrix}
4 & 0 \\
0 & 4
\end{bmatrix}\text{only similiar to itself}
$$
$$
M^{-1} 
\begin{bmatrix}
4 & 0 \\
0 & 4
\end{bmatrix}
M =
\begin{bmatrix}
4 & 0 \\
0 & 4
\end{bmatrix}
$$

Another big family has
$$
\begin{bmatrix}
4 & 1 \\
0 & 4
\end{bmatrix}
$$
iT is the **Jordan form** of A
more members in the family
$$
\begin{bmatrix}
4 & 1 \\
0 & 4
\end{bmatrix}
\begin{bmatrix}
5 & 1 \\
-1 & 3
\end{bmatrix}
\begin{bmatrix}
4 & 0 \\    
17 & 4
\end{bmatrix}
$$
- trace = 8,det = 16
- only have one eigenvector

Another example:
$$
A_1 =
\begin{bmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}
$$
- eigenvalues are 0,0,0,0
- rank = 2, dim of nullspace = 2
change it
$$
\begin{bmatrix}
0 & 1 & 7 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}
$$
- same 
- 2 missing eigenvectors

Example2:
$$
A_2 =
\begin{bmatrix}
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 0
\end{bmatrix}
$$
- eigenvalues are 0,0,0,0, rank = 2, dim of nullspace = 2
- **not similiar** to the previous one
because Jordan block is different
#### Jordan block
$$
J_i =
\begin{bmatrix}
\lambda_i & 1 & 0 & 0 \\
0 & \lambda_i & 1 & 0 \\
0 & 0 & \lambda_i & 1 \\
0 & 0 & 0 & \lambda_i
\end{bmatrix}
$$
- zeros below, one above
- only have one eigenvector

SO $A_1$ and $A_2$ are not similiar

#### Every matrix is similiar to a Jordan matrix $J$
$$
J = 
\begin{bmatrix}
J_1 & 0 & 0 \\
0 & J_2 & 0 \\
0 & 0 & J_3
\end{bmatrix}
$$
- the number of Jordan blocks is the number of eigenvectors


## 29. Singular value decomposition (SVD)
$$
A = U \Sigma V^T
$$
- A is any matrix
- U and V are orthogonal
> a photo: U is the camera, V is the projector
> $\sigma_1 u_1 = A v_1$
$$
A 
\begin{bmatrix}
v_1 & v_2 & v_3 & \cdots &v_m \\
\end{bmatrix} =
\begin{bmatrix}
u_1 & u_2 & u_3 & \cdots &u_n \\
\end{bmatrix}
\begin{bmatrix}
\sigma_1 & 0 & 0 & \cdots &0 \\
0 & \sigma_2 & 0 & \cdots &0 \\
0 & 0 & \sigma_3 & \cdots &0 \\
\end{bmatrix}
$$
> $\sigma$ and $v$ are the basis vectors

A times the first basis vectors should be $\sigma$ times  another basis vectors
- v is the basis of the row space
- u is the basis of the column space

Express as matrics:
$$
A V = U \Sigma
$$

Example:
$$
A = 
\begin{bmatrix}
4 & 4 \\
-3 & 3
\end{bmatrix}
$$
- $v_1, v_2$ in row space $R^2$
- $u_1, u_2$ in column space $R^2$
- both are orthogonal
- $\sigma_1, \sigma_2$ >0

So we hope:
- $A v_1 = \sigma_1 u_1$
- $A v_2 = \sigma_2 u_2$

$$
A  = U \Sigma V^{-1} = U \Sigma V^T
$$
SO 
$$
A^T A = V \Sigma^T U^T U \Sigma V^T = V \Sigma^T \Sigma V^T
$$
- $U^T V = I$
$$
= V 
\begin{bmatrix}
\sigma_1^2 & 0 \\
0 & \sigma_2^2
\end{bmatrix}
V^T
$$
- V are the eigenvectors of $A^T A$
- $A^T A$ is symmetric and positive definite


#### How to find SVD
1. compute $A^T A$
$$
A^T A =
\begin{bmatrix}
4 & -3 \\
4 & 3
\end{bmatrix}
\begin{bmatrix}
4 & 4 \\
-3 & 3
\end{bmatrix}
=
\begin{bmatrix}
25 & 7 \\  
7 & 25
\end{bmatrix}
$$
eigenvalues and eigenvectors?

$$
25
\begin{bmatrix}
1 \\
1
\end{bmatrix}, 18
\begin{bmatrix}
1 \\
-1
\end{bmatrix}
$$
so 
... todo
find V
2. compute $A A^T$
find U

Example2:
$$
A =
\begin{bmatrix}
4 & 3 \\
8 & 6
\end{bmatrix}
$$
- row space is 1D,in (4,3) direction
- column space is 1D,in (4,8) direction

- only one $V_1$ and $U_1$
$v_1 = \begin{bmatrix}
0.8 \\
0.6
\end{bmatrix},
u_1 = \frac{1}{\sqrt{5}}\begin{bmatrix}
1 \\
2
\end{bmatrix}$
SO 
$$
A = U \Sigma V^T =\frac{1}{\sqrt{5}}
\begin{bmatrix}
1 & 2 \\
2 & -1
\end{bmatrix}
\begin{bmatrix}
\sqrt{125} & 0 \\
0 & 0   
\end{bmatrix}
\begin{bmatrix}
0.8 & 0.6 \\
0.6 & -0.8
\end{bmatrix}
$$

#### Conclusion
1. $v_1 ... v_r$ are the basis of the row space
2. $u_1 ... u_r$ are the basis of the column space
3. $v_{r+1} ... v_m$ are the basis of the nullspace
4. $u_{r+1} ... u_n$ are the basis of the nullspace OF $A^T$
   

## 30. Linear transformations
- $T: R^2 \rightarrow R^2$
  - like a function, an input and an output
- $T(v+w    ) = T(v) + T(w)$
- $T(cv) = c T(v)$
- $T(cv+dw) = c T(v) + d T(w)$
  - shifting in plane is not linear transformation


Unstand linear transformation:Find T matrix

first :
start :
$T$:$R^3$-> $R^2$
example:
$$
T(v) = Av 
$$
   - v: input vector in $R^3$
   - T(v): output vector in $R^2$
   - A: matrix of T, 2 by 3 matrix

if $v_1$ and $v_2$ are not dependent, finding  $T(v_1)$and $T(v_2)$ means  we know the output of T
because every v is a linear combination of $v_1$ and $v_2$
$$
v = 
\begin{bmatrix}
3 \\ 2 \\ 4
\end{bmatrix} = 3 
\begin{bmatrix}
1 \\ 0 \\ 0
\end{bmatrix} + 2
\begin{bmatrix}
0 \\ 1 \\ 0
\end{bmatrix} + 4
\begin{bmatrix}
0 \\ 0 \\ 1
\end{bmatrix}
$$


$T$: $R^n$-> $R^m$
Choose basis $v_1, v_2, ... v_n$ of $R^n$
$w_1, w_2, ... w_m$ of $R^m$


$v_1, v_2$ are the basis of input and output space

$v = c_1 v_1 + c_2 v_2$

$$
T(v) = c_1 v_1
$$

find c: 
$(c_1, c_2) \rightarrow (c_1, 0)$


$$
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
c_1 \\
c_2
\end{bmatrix}=
\begin{bmatrix}
c_1 \\
0 
\end{bmatrix}
$$
$$
A * \text{input coords} = \text{output coords}
$$

### Rule to find A
> given basis of input v and output w
1. first column of A 
    - write $T(v_1)=a_{11}w_1+a_{m1}w_2 ... $ 
2. second column of A 
    - write $T(v_2)=a_{12}w_1+a_{m2}w_2 ... $
$$
A\begin{bmatrix}
\text{input coords} 
\end{bmatrix} = \begin{bmatrix}
\text{output coords}    
\end{bmatrix}
$$

### linear transformation to take derivative
Input: $c_1+c_2x+c_3x^2$
input basis : $1, x, x^2$

output: $c_2+c_3x$
output basis : $1, x$
So
$$
\begin{bmatrix}
0 & 1 0 \\
0 & 0 & 2 \\
0 & 0 & 0
\end{bmatrix} 
\begin{bmatrix}
c_1 \\ c_2 \\ c_3
\end{bmatrix} =
\begin{bmatrix}
c_2 \\ 2c_3  \end{bmatrix}
$$

## 31. Change of basis, Compression of images
- Image compression:
Fourier basis
> 略


#### Change of basis:
columns of W = new basis vectors 
$$
\begin{bmatrix}
x 
\end{bmatrix}_{old basis} = 
\begin{bmatrix}
c 
\end{bmatrix}_{new basis}
$$
$$
x = Wc
$$
T with the respect to v_1, v_2, ... v_n, it has matrix A
with respect to w_1, w_2, ... w_m, it has matrix B
Relation bretween A and B?
- they are **similiar**
> $B = M^{-1} A M$, M is the **change of basis matrix**

What is A?
using the basis of $v_1, v_2, ... v_8$ to express the output
know T completely from $T(v_1), T(v_2), ... T(v_8)$
then $T(x) = c_1T(v_1)+c_2T(v_2)...$ 

write $T(v_1) = a_{11}v_1 + a_{12}v_2 + ... + a_{81}v_8$
$T(v_2) = a_{21}v_1 + a_{22}v_2 + ... + a_{82}v_8$
SO
$$
[A] = \begin{bmatrix}
a_{11} & a_{12} & ... & a_{18} \\
a_{21} & a_{22} & ... & a_{28} \\
\vdots & \vdots & \ddots & \vdots \\
a_{81} & a_{82} & ... & a_{88}
\end{bmatrix}
$$

Eigenvector basis:
$$
T(v_i)= \lambda_i v_i
$$
what is A?
$$
A = \begin{bmatrix}
\lambda_1 & 0 & ... & 0 \\
0 & \lambda_2 & ... & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & ... & \lambda_n
\end{bmatrix}
$$

## 33. Inverse（left,right,pseudo）
$$
A A^{-1} = I = A^{-1} A
$$
when r = m = n , **full rank**

#### not full rank?
- full column rank: have a **left** inverse
  - r = n, m > n
> if we have full column rank, we can find a left inverse

$$
\text{nullspace}(A) = {0}
$$
0 or 1 solutions to $Ax = b$


" one side inverse"
- left inverse:
$$
(A^T A)^{-1} A^T A = I
$$
$$
A^{-1}_{n*m} A_{m*n} = I_{n*n}
$$
> a rectangular matrix can't have a two side inverse

- right inverse:
full row rank :$r = m < n$
$$
n(A^T)=0
$$

$$
A A^T (A A^T)^{-1} = I
$$
$$
A A^{-1} = 0
$$
if a times the left inverse
$$
A(A^T A)^{-1} A^T = P
$$
> P is the projection matrix

it is a projection onto the column space of A

- pseudo inverse:
